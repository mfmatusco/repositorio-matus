{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Reducción datos archivo original.** Esto con intención de tener la primera base de datos para la visualización de ***ARTISTAS + SELLOS DISCOGRAFICOS.***"
      ],
      "metadata": {
        "id": "YvApwgz-HuOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. *En función de evidenciar el dominio de ciertas empresas en el mercado estadounidense*, **se realiza un aislamiento de datos**, teniendo como enfoque el top 10 del Hot 100, por ende considerango el rango de añs como 2000-2025, **se edita el csv completo para quedar con las canciones separando a los artistas de forma individual**. ejemplo: Golden de Huntrix ahora se desgloza asociado a las 3 vocalistas de forma separada."
      ],
      "metadata": {
        "id": "BiRqSlTYzEo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/csv/hot100_top10_only.csv\")\n",
        "\n",
        "df_artists = df[['Date', 'Song', 'Artist']].copy()\n",
        "\n",
        "df_artists['Artist_Clean'] = (df_artists['Artist']\n",
        "                              .str.replace(r'^\"|\"$', '', regex=True)\n",
        "                              .str.replace(r'\"\"', '\"', regex=True)\n",
        "                             )\n",
        "\n",
        "df_artists['Artist_Clean'] = (df_artists['Artist_Clean']\n",
        "                              .str.replace(r'\\s+Featuring\\s+', '|', regex=True)\n",
        "                              .str.replace(r'\\s+&\\s+', '|', regex=True)\n",
        "                              .str.replace(r'\\s+with\\s+', '|', regex=True)\n",
        "                              .str.replace(r':\\s+', '|', regex=True)\n",
        "                             )\n",
        "\n",
        "df_artists['Individual_Artist'] = df_artists['Artist_Clean'].str.split('|')\n",
        "df_artists_exploded = df_artists.explode('Individual_Artist')\n",
        "\n",
        "\n",
        "df_artists_exploded['Individual_Artist'] = df_artists_exploded['Individual_Artist'].str.strip()\n",
        "df_artists_only = df_artists_exploded[['Date', 'Song', 'Individual_Artist']].rename(columns={'Individual_Artist': 'Artist'})\n",
        "df_artists_only = df_artists_only.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "df_artists_only.to_csv(\"individual_artists.csv\", index=False)"
      ],
      "metadata": {
        "id": "cGXYNUXgzE_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ahora se ejecuta el codigo para incorporar los sellos discograficos a cada artista. Con esto realizado, el archivo seria ya base para las visualizaciones."
      ],
      "metadata": {
        "id": "xREiu8sM1ZZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import urllib.parse\n",
        "\n",
        "\n",
        "df_artists_only = pd.read_csv(\"/content/drive/MyDrive/csv/individual_artists.csv\")\n",
        "\n",
        "df_artists_only['Record_Label'] = None\n",
        "\n",
        "\n",
        "GENIUS_ACCESS_TOKEN = \"e87LgpoSjehzdKaXfFTU1USopb4yCPNI047aAgqd-OQEacCxNd_ybcll3wIxvj4P\"\n",
        "BASE_URL = \"https://api.genius.com/\"\n",
        "HEADERS = {\n",
        "    'Authorization': f'Bearer {GENIUS_ACCESS_TOKEN}',\n",
        "}\n",
        "\n",
        "for index, row in df_artists_only.iterrows():\n",
        "    artist = row['Artist']\n",
        "    song = row['Song']\n",
        "\n",
        "    search_query = f\"{song} {artist}\"\n",
        "\n",
        "    try:\n",
        "        search_endpoint = BASE_URL + \"search\"\n",
        "        params = {'q': search_query}\n",
        "\n",
        "        response = requests.get(search_endpoint, headers=HEADERS, params=params)\n",
        "        response.raise_for_status()\n",
        "        search_data = response.json()\n",
        "\n",
        "\n",
        "        song_id = None\n",
        "        if search_data.get('response', {}).get('hits'):\n",
        "            best_hit = search_data['response']['hits'][0]['result']\n",
        "            song_id = best_hit['id']\n",
        "\n",
        "\n",
        "            if song_id:\n",
        "                song_endpoint = BASE_URL + f\"songs/{song_id}\"\n",
        "\n",
        "                response_song = requests.get(song_endpoint, headers=HEADERS)\n",
        "                response_song.raise_for_status()\n",
        "                song_data = response_song.json()\n",
        "\n",
        "\n",
        "                title = song_data['response']['song']['title_with_featured']\n",
        "                df_artists_only.loc[index, 'Record_Label'] = f\"Datos_obtenidos ({title})\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        df_artists_only.loc[index, 'Record_Label'] = f\"Error_API: {e}\"\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "df_artists_only.to_csv(\"artistas_genius_status.csv\", index=False)"
      ],
      "metadata": {
        "id": "UfKUxtv2B7lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al final, debido a lo masivo de la cantidad de datos y el continuo error, no segui trabajando con este.\n",
        "\n",
        "Por otro lado, a pesar de en pruebas haber llegado al resultado deseado de sellos + artistas, entre el archivo pasado por limpieza y el que se le sumaron sellos, al ultimo le faltaron varios artistas. Por lo que su uso implicaria un verdad imcopleta."
      ],
      "metadata": {
        "id": "3xWhwIz6ZpJi"
      }
    }
  ]
}